{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8327c380c5fc799",
   "metadata": {},
   "source": [
    "# ASR Benchmarking Notebook for Multitask-National-Speech-Corpus-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9cf495f2ac8268dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.536946Z",
     "start_time": "2025-04-26T15:34:36.798362Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq, WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer, mer, wip, wil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42835e108ea33568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.574726Z",
     "start_time": "2025-04-26T15:34:41.543716Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Configurations -----------------------\n",
    "MODEL_ID = \"openai/whisper-small\"  # Changeable to local or finetuned models\n",
    "DATA_DIR = \"ASR-PART2-Test\"        # Changeable for different partitions\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_SAMPLES = 100                  # Number of samples for quick benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a28e19f9fe22e1a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.665928Z",
     "start_time": "2025-04-26T15:34:41.663219Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Load Dataset -----------------------\n",
    "def load_data(data_dir, split='train', num_samples=None):\n",
    "    dataset = load_dataset('MERaLiON/Multitask-National-Speech-Corpus-v1', data_dir=data_dir)[split]\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ed74e4f5709547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.711229Z",
     "start_time": "2025-04-26T15:34:41.707627Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Load ASR Model -----------------------\n",
    "def load_asr_model(model_id):\n",
    "    try:\n",
    "        # Try using a generic pipeline-based ASR (e.g. Whisper, wav2vec2)\n",
    "        asr_pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model_id,\n",
    "            device=0 if DEVICE == \"cuda\" else -1\n",
    "        )\n",
    "\n",
    "        def wrapped_pipeline(audio_array, sampling_rate=16000):\n",
    "            return asr_pipeline(audio_array)\n",
    "\n",
    "        return wrapped_pipeline\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Falling back to custom processor loading for model {model_id} due to: {e}\")\n",
    "        # Custom processor and model logic\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(DEVICE)\n",
    "\n",
    "        def custom_asr_pipeline(audio_array, sampling_rate=16000):\n",
    "            input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(input_features=input_features)\n",
    "            transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return {\"text\": transcription}\n",
    "\n",
    "        return custom_asr_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9d5f9f24abf45bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.759865Z",
     "start_time": "2025-04-26T15:34:41.755599Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Benchmarking Function -----------------------\n",
    "def benchmark_asr(asr_pipeline, dataset):\n",
    "    results = []\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Transcribing\"):\n",
    "        audio_array = example[\"context\"][\"array\"]\n",
    "        reference = example[\"answer\"]\n",
    "\n",
    "        start = time.time()\n",
    "        prediction = asr_pipeline(audio_array, sampling_rate=16000)[\"text\"]\n",
    "        end = time.time()\n",
    "\n",
    "        results.append({\n",
    "            \"reference\": reference,\n",
    "            \"prediction\": prediction,\n",
    "            \"time\": end - start\n",
    "        })\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d9b27ffa129f35e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.812206Z",
     "start_time": "2025-04-26T15:34:41.806955Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Evaluation Metrics -----------------------\n",
    "def evaluate_metrics(result_df):\n",
    "    result_df[\"wer\"] = result_df.apply(lambda x: wer(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"mer\"] = result_df.apply(lambda x: mer(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"wil\"] = result_df.apply(lambda x: wil(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"wip\"] = result_df.apply(lambda x: wip(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "\n",
    "    average_metrics = {\n",
    "        \"Average WER\": result_df[\"wer\"].mean(),\n",
    "        \"Average MER\": result_df[\"mer\"].mean(),\n",
    "        \"Average WIL\": result_df[\"wil\"].mean(),\n",
    "        \"Average WIP\": result_df[\"wip\"].mean(),\n",
    "        \"Average Time per Sample (s)\": result_df[\"time\"].mean()\n",
    "    }\n",
    "\n",
    "    return result_df, average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "476ddff3520b996b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.860038Z",
     "start_time": "2025-04-26T15:34:41.855565Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Identify Best/Worst Words -----------------------\n",
    "def word_analysis(result_df):\n",
    "    from collections import Counter\n",
    "    word_errors = []\n",
    "    for _, row in result_df.iterrows():\n",
    "        ref_words = row[\"reference\"].lower().split()\n",
    "        pred_words = row[\"prediction\"].lower().split()\n",
    "        missed = set(ref_words) - set(pred_words)\n",
    "        for word in missed:\n",
    "            word_errors.append(word)\n",
    "\n",
    "    error_counter = Counter(word_errors)\n",
    "    most_common_errors = error_counter.most_common(10)\n",
    "    least_common_errors = error_counter.most_common()[-10:]\n",
    "    return most_common_errors, least_common_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1e7e36a60f042748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.915463Z",
     "start_time": "2025-04-26T15:34:41.904208Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from jiwer import wer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the reference text by:\n",
    "    - Removing speaker labels like <speaker1>: or <speaker2>:\n",
    "    - Removing round or square brackets but keeping the content inside.\n",
    "    - Converting to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove speaker tags like <speaker1>: or <speaker2>:\n",
    "    text = re.sub(r\"<speaker\\d+>:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove round or square brackets but keep content inside\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\((.*?)\\)\", r\"\\1\", text)\n",
    "\n",
    "    # Convert to lowercase and strip surrounding whitespace\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def run_benchmark(model_id, data_dir=\"ASR-PART1-Train\", num_samples=100, batch_size=8):\n",
    "    print(f\"\\n📊 Running benchmark for model: {model_id}\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"MERaLiON/Multitask-National-Speech-Corpus-v1\", data_dir=data_dir)[\"train\"]\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    # Load ASR model pipeline\n",
    "    model_fn = load_asr_model(model_id)\n",
    "\n",
    "    # Prepare audio and references\n",
    "    audio_arrays = [sample[\"context\"][\"array\"] for sample in dataset]\n",
    "    references = [sample[\"answer\"] for sample in dataset]\n",
    "\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run batched inference\n",
    "    for i in tqdm(range(0, len(audio_arrays), batch_size), desc=f\"Evaluating {model_id}\", ncols=100):\n",
    "        batch_audio = audio_arrays[i:i+batch_size]\n",
    "        batch_refs = references[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            # Support both batch and single-audio pipelines\n",
    "            preds = model_fn(batch_audio)\n",
    "\n",
    "            # Ensure output is iterable\n",
    "            if isinstance(preds, dict):\n",
    "                preds = [preds]\n",
    "\n",
    "            for ref, pred_dict in zip(batch_refs, preds):\n",
    "                hyp = pred_dict.get(\"text\", \"[ERROR]\").strip()\n",
    "\n",
    "                # Clean both reference and prediction\n",
    "                ref_cleaned = clean_text(ref)\n",
    "                hyp_cleaned = clean_text(hyp)\n",
    "\n",
    "                predictions.append({\"reference\": ref_cleaned, \"prediction\": hyp_cleaned})\n",
    "\n",
    "        except Exception as e:\n",
    "            for ref in batch_refs:\n",
    "                predictions.append({\"reference\": clean_text(ref), \"prediction\": f\"[ERROR: {e}]\"})\n",
    "\n",
    "        # Dynamically print progress without breaking the progress bar\n",
    "        tqdm.write(f\"Processed {i + batch_size}/{len(audio_arrays)} samples.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Compute metrics\n",
    "    references_clean = [x[\"reference\"] for x in predictions]\n",
    "    predictions_clean = [x[\"prediction\"] for x in predictions]\n",
    "    computed_wer = wer(references_clean, predictions_clean)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n✅ Benchmark complete for {model_id}\")\n",
    "    print(f\"⏱️ Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"🧠 WER: {computed_wer:.4f}\")\n",
    "\n",
    "    # Sample output\n",
    "    print(\"\\n🔍 Sample predictions:\")\n",
    "    for sample in predictions[:5]:\n",
    "        print(f\"REF: {sample['reference']}\")\n",
    "        print(f\"HYP: {sample['prediction']}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"wer\": computed_wer,\n",
    "        \"runtime_sec\": total_time,\n",
    "        \"results\": predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f7dc245ce2e555c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:44:01.255986Z",
     "start_time": "2025-04-26T15:34:43.420731Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Multi-Model Benchmark Loop -----------------------\n",
    "import gc\n",
    "\n",
    "def benchmark_multiple_models(model_ids, data_dir=\"ASR-PART5-Test\", num_samples=100, batch_size=8):\n",
    "    comparison_results = []\n",
    "\n",
    "    for model_id in model_ids:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() # Clear the GPU cache\n",
    "\n",
    "        print(f\"\\n===== Benchmarking Model: {model_id} =====\")\n",
    "\n",
    "        # Run benchmark and get results\n",
    "        result = run_benchmark(model_id=model_id, data_dir=data_dir, num_samples=num_samples, batch_size=batch_size)\n",
    "\n",
    "        # Collecting metrics for comparison\n",
    "        metrics = {\"WER\": result[\"wer\"], \"Runtime (sec)\": result[\"runtime_sec\"], \"Model\": model_id}\n",
    "        comparison_results.append(metrics)\n",
    "\n",
    "    # Convert list of dicts to DataFrame for easy comparison\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    comparison_df = comparison_df.set_index(\"Model\")\n",
    "\n",
    "    return comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b0bf4a4b533b3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# model_list = [\n",
    "#     \"openai/whisper-medium.en\",\n",
    "#     \"openai/whisper-small.en\",\n",
    "#     \"openai/whisper-tiny.en\",\n",
    "#     \"openai/whisper-base.en\",\n",
    "#     \"facebook/wav2vec2-base-960h\",\n",
    "#     \"jensenlwt/whisper-small-singlish-122k\",\n",
    "#     # \"path/to/your/local/model\",  # Example for a local model\n",
    "# ]\n",
    "\n",
    "# comparison_df = benchmark_multiple_models(model_list)\n",
    "# display(comparison_df.sort_values(by=\"WER\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9385c976861cf7a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:08:35.236698Z",
     "start_time": "2025-04-26T15:03:15.049549Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# model_list = [\n",
    "#     \"mjwong/whisper-large-v3-turbo-singlish\",\n",
    "# ]\n",
    "\n",
    "# comparison_df_2 = benchmark_multiple_models(model_list)\n",
    "# display(comparison_df_2.sort_values(by=\"WER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e0070d17030c7f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:44:14.750609Z",
     "start_time": "2025-04-26T15:44:01.296701Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# model_list = [\n",
    "#     \"mjwong/whisper-large-v3-singlish\",\n",
    "#     \"mjwong/whisper-small-singlish\",\n",
    "# ]\n",
    "\n",
    "# comparison_df_3 = benchmark_multiple_models(model_list)\n",
    "# display(comparison_df_3.sort_values(by=\"WER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c98df652accba471",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEPINFRA_API_KEY = \"50J6Cq9Wl0b1nFaouy8LxnyA5CdAXIt7\"  # Get from DeepInfra dashboard\n",
    "DEEPINFRA_API_BASE = \"https://api.deepinfra.com/v1/openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d5791553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_with_llm(hypotheses, llm_type=\"meta-llama/Llama-3-70b-chat-hf\"):\n",
    "    \"\"\"Use DeepInfra-hosted LLM for correction\"\"\"\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=DEEPINFRA_API_KEY,\n",
    "        base_url=DEEPINFRA_API_BASE\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"Correct this speech transcription, which is in Singlish and likely to contain words related to Singapore. You may need to correct it based on the knowledge you have about Singapore, but you shall correct it only when it's necessary. Using these ASR hypotheses and their confidence scores:\n",
    "    \n",
    "    {chr(10).join([f'- {h[\"text\"]} (confidence: {h[\"confidence\"]:.2f})' for h in hypotheses])}\n",
    "    \n",
    "    Do not include any other text or explanation. Just provide the corrected transcript.\"\"\"\n",
    "    #\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_type,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0980cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_asr_hypotheses(asr_pipe, audio_array, num_hypotheses=3):\n",
    "    \"\"\"\n",
    "    Generate ASR hypotheses with confidence scores for the given audio.\n",
    "    \n",
    "    Args:\n",
    "        asr_pipe: The ASR pipeline function\n",
    "        audio_array: Audio array to transcribe\n",
    "        num_hypotheses: Number of hypotheses to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with text and score\n",
    "    \"\"\"\n",
    "    hypotheses = []\n",
    "    \n",
    "    try:\n",
    "        # For Whisper models, we can use the return_timestamps and beam search\n",
    "        if hasattr(asr_pipe, \"model\") and \"whisper\" in str(asr_pipe.model.__class__).lower():\n",
    "            # Get transcription with beam search to have multiple candidates\n",
    "            result = asr_pipe(\n",
    "                audio_array,\n",
    "                return_timestamps=False,\n",
    "                generate_kwargs={\"num_beams\": num_hypotheses, \"num_return_sequences\": num_hypotheses}\n",
    "            )\n",
    "            \n",
    "            # If result is a list of candidates\n",
    "            if isinstance(result, list):\n",
    "                for i, res in enumerate(result):\n",
    "                    # Normalize scores based on position in beam search results\n",
    "                    score = 1.0 - (i * 0.1)  # Simple decay\n",
    "                    hypotheses.append({\"text\": clean_text(res.get(\"text\", \"\")), \"score\": score})\n",
    "            else:\n",
    "                # If single result with chunks\n",
    "                if \"chunks\" in result:\n",
    "                    # Use chunk confidence if available\n",
    "                    for chunk in result[\"chunks\"]:\n",
    "                        hypotheses.append({\n",
    "                            \"text\": clean_text(chunk.get(\"text\", \"\")),\n",
    "                            \"score\": chunk.get(\"probability\", 0.9)\n",
    "                        })\n",
    "                else:\n",
    "                    # Fallback to single hypothesis\n",
    "                    hypotheses.append({\"text\": clean_text(result.get(\"text\", \"\")), \"score\": 0.9})\n",
    "        else:\n",
    "            # For non-Whisper models, we try to get word-level confidence if available\n",
    "            try:\n",
    "                # First get standard transcription\n",
    "                result = asr_pipe(audio_array)\n",
    "                \n",
    "                # Try to extract model and processor for detailed confidences\n",
    "                if hasattr(asr_pipe, \"model\"):\n",
    "                    model = asr_pipe.model\n",
    "                    processor = asr_pipe.tokenizer or asr_pipe.feature_extractor\n",
    "                    \n",
    "                    # Process audio with feature extractor\n",
    "                    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\").to(model.device)\n",
    "                    \n",
    "                    # Get logits with output token probabilities\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                    \n",
    "                    # Normalize logits to get probabilities\n",
    "                    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                    \n",
    "                    # Get the maximum probability for each prediction step\n",
    "                    confidence = torch.max(probs, dim=-1).values.mean().item()\n",
    "                    \n",
    "                    base_text = result.get(\"text\", \"\") if isinstance(result, dict) else str(result)\n",
    "                    hypotheses.append({\"text\": clean_text(base_text), \"score\": confidence})\n",
    "                else:\n",
    "                    # Fallback if we can't access model details\n",
    "                    base_text = result.get(\"text\", \"\") if isinstance(result, dict) else str(result)\n",
    "                    hypotheses.append({\"text\": clean_text(base_text), \"score\": 0.8})\n",
    "            except:\n",
    "                # Final fallback - use standard output with estimated confidence\n",
    "                if isinstance(result, dict):\n",
    "                    base_text = result.get(\"text\", \"\").strip()\n",
    "                elif isinstance(result, list) and len(result) > 0:\n",
    "                    base_text = result[0].get(\"text\", \"\").strip()\n",
    "                else:\n",
    "                    base_text = str(result).strip()\n",
    "                \n",
    "                # Simple confidence heuristic based on text length\n",
    "                length_factor = min(1.0, len(base_text) / 100)  # Assumes longer texts are more confident\n",
    "                hypotheses.append({\"text\": clean_text(base_text), \"score\": 0.7 + (length_factor * 0.2)})\n",
    "                \n",
    "            # If we only have one hypothesis but need more\n",
    "            if len(hypotheses) < num_hypotheses:\n",
    "                base_text = hypotheses[0][\"text\"]\n",
    "                base_score = hypotheses[0][\"score\"]\n",
    "                \n",
    "                # Generate variants by randomly removing or perturbing words\n",
    "                import random\n",
    "                words = base_text.split()\n",
    "                \n",
    "                for i in range(1, num_hypotheses):\n",
    "                    if len(words) > 3:\n",
    "                        # Remove a random word\n",
    "                        variant_words = words.copy()\n",
    "                        remove_idx = random.randint(0, len(variant_words) - 1)\n",
    "                        variant_words.pop(remove_idx)\n",
    "                        variant_text = \" \".join(variant_words)\n",
    "                        \n",
    "                        # Score decays with each variant\n",
    "                        variant_score = base_score * (0.9 ** i)\n",
    "                        hypotheses.append({\"text\": variant_text, \"score\": variant_score})\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Error handling\n",
    "        hypotheses = [{\"text\": f\"[ERROR: {e}]\", \"score\": 0.1}]\n",
    "    \n",
    "    # Ensure we return the requested number of hypotheses\n",
    "    while len(hypotheses) < num_hypotheses:\n",
    "        if len(hypotheses) > 0:\n",
    "            # Create variants of the first hypothesis\n",
    "            base = hypotheses[0][\"text\"]\n",
    "            score = hypotheses[0][\"score\"] * 0.7  # Reduce confidence\n",
    "            hypotheses.append({\"text\": base, \"score\": score})\n",
    "        else:\n",
    "            # Complete fallback\n",
    "            hypotheses.append({\"text\": \"[Unknown transcription]\", \"score\": 0.1})\n",
    "    \n",
    "    # Return only the requested number\n",
    "    return hypotheses[:num_hypotheses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fdbfd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def generate_whisper_hypotheses(\n",
    "    audio_array,\n",
    "    model_id=\"openai/whisper-small.en\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    sampling_rate=16000,\n",
    "    num_hypotheses=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate Whisper ASR hypotheses with one confidence score each.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: [{\"text\": str, \"confidence\": float}, ...]\n",
    "    \"\"\"\n",
    "    # 1. Load model & processor\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "    processor = WhisperProcessor.from_pretrained(model_id)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2. Prepare input features\n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "    features = inputs.input_features.to(device)\n",
    "\n",
    "    # 3. Generate N-best sequences with scores\n",
    "    outputs = model.generate(\n",
    "        features,\n",
    "        do_sample=True,            # enable sampling\n",
    "        num_return_sequences=3,    # sample 3 times\n",
    "        max_new_tokens=256,\n",
    "        top_k=50,                  \n",
    "        top_p=0.95,                # or nucleus sampling\n",
    "        temperature=0.7,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    hypotheses = []\n",
    "    for idx in range(num_hypotheses):\n",
    "        seq_ids = outputs.sequences[idx]\n",
    "        text = processor.decode(seq_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # 4a. If HF exposes beam scores directly, use them\n",
    "        if hasattr(outputs, \"sequences_scores\") and outputs.sequences_scores is not None:\n",
    "            sum_logprob = outputs.sequences_scores[idx]\n",
    "        else:\n",
    "            # 4b. Otherwise sum per-step log-probs manually\n",
    "            sum_logprob = 0.0\n",
    "            # outputs.scores: tuple of (batch_size*num_beams, vocab_size) at each step\n",
    "            for step, step_scores in enumerate(outputs.scores):\n",
    "                logprobs = torch.log_softmax(step_scores, dim=-1)\n",
    "                token_id = seq_ids[step + 1]  # skip <s> token\n",
    "                # beam i is at index idx in the (batch_size*num_beams) dimension\n",
    "                sum_logprob += logprobs[idx, token_id]\n",
    "\n",
    "        # 5. Compute average log-prob per token (exclude the initial token)\n",
    "        token_count = seq_ids.shape[-1] - 1\n",
    "        avg_logprob = (sum_logprob / token_count).item()\n",
    "\n",
    "        hypotheses.append({\"text\": text, \"confidence\": avg_logprob})\n",
    "\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0144b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Enhanced Benchmarking with Progress -----------------------\n",
    "def benchmark_llm_correction(asr_model_id, llm_models, num_samples=50):\n",
    "    dataset = load_data(DATA_DIR, num_samples=num_samples)\n",
    "    #asr_pipe = load_asr_model(asr_model_id)\n",
    "    \n",
    "    results = []\n",
    "    total_models = len(llm_models)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    with tqdm(total=total_models * total_samples, desc=\"Overall Progress\") as main_pbar:\n",
    "        for llm_idx, llm in enumerate(llm_models):\n",
    "            model_progress = {\n",
    "                \"processed\": 0,\n",
    "                \"examples\": []\n",
    "            }\n",
    "            \n",
    "            with tqdm(dataset, desc=f\"Model {llm_idx+1}/{total_models}: {llm[:20]}...\") as model_pbar:\n",
    "                corrected = []\n",
    "                for sample_idx, example in enumerate(dataset):\n",
    "                    # Generate ASR hypotheses\n",
    "                    #hypotheses = generate_asr_hypotheses(asr_pipe, example[\"context\"][\"array\"])\n",
    "                    hypotheses = generate_whisper_hypotheses(example[\"context\"][\"array\"], model_id=asr_model_id)\n",
    "                    # LLM Correction\n",
    "                    corrected_text = correct_with_llm(hypotheses, llm)\n",
    "                    \n",
    "                    # Store results\n",
    "                    corrected.append({\n",
    "                        \"original\": hypotheses[0][\"text\"],\n",
    "                        \"corrected\": clean_text(corrected_text),\n",
    "                        \"reference\": clean_text(example[\"answer\"])\n",
    "                    })\n",
    "                    \n",
    "                    # Update progress\n",
    "                    model_progress[\"processed\"] += 1\n",
    "                    main_pbar.update(1)\n",
    "                    model_pbar.update(1)\n",
    "                    \n",
    "                    # Show examples every 10% of samples\n",
    "                    if (sample_idx + 1) % max(1, total_samples//10) == 0:\n",
    "                        progress_percent = int(100 * (sample_idx + 1) / total_samples)\n",
    "                        latest_example = corrected[-1]\n",
    "                        \n",
    "                        example_output = f\"\"\"\n",
    "                        \\n=== [Model: {llm} | Progress: {progress_percent}%] ===\n",
    "                        [Original] {latest_example['original']}\n",
    "                        [Corrected] {latest_example['corrected']}\n",
    "                        [Reference] {latest_example['reference']}\n",
    "                        \"\"\"\n",
    "                        tqdm.write(example_output)\n",
    "                        model_progress[\"examples\"].append(example_output)\n",
    "\n",
    "                # Calculate metrics\n",
    "                wer_original = wer([c[\"reference\"] for c in corrected], [c[\"original\"] for c in corrected])\n",
    "                wer_corrected = wer([c[\"reference\"] for c in corrected], [c[\"corrected\"] for c in corrected])\n",
    "                \n",
    "                results.append({\n",
    "                    \"LLM\": llm,\n",
    "                    \"Original WER\": wer_original,\n",
    "                    \"Corrected WER\": wer_corrected,\n",
    "                    \"Improvement\": wer_original - wer_corrected,\n",
    "                    \"Examples\": model_progress[\"examples\"]\n",
    "                })\n",
    "\n",
    "    # Final output with examples\n",
    "    print(\"\\n=== Benchmark Complete ===\")\n",
    "    for result in results:\n",
    "        print(f\"\\nModel: {result['LLM']}\")\n",
    "        print(f\"WER Improvement: {result['Improvement']:.3f}\")\n",
    "        print(\"Sample Corrections:\")\n",
    "        for example in result['Examples'][::2]:  # Show every other example\n",
    "            print(example)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7a9f93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/30 [00:00<?, ?it/s]d:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_scores` is. When `return_dict_in_generate` is not `True`, `output_scores` is ignored.\n",
      "  warnings.warn(\n",
      "Overall Progress:  10%|█         | 3/30 [00:17<02:33,  5.70s/it]\n",
      "Overall Progress:  10%|█         | 3/30 [00:17<02:33,  5.70s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 10%] ===\n",
      "                        [Original] Castle, Tender, Beef, Raderoo, and Hellmuth.\n",
      "                        [Corrected] cassu, tender beef, radarou and hellmuths.\n",
      "                        [Reference] katsu tendon beef vindaloo and hummus\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  20%|██        | 6/30 [00:33<02:13,  5.56s/it]\n",
      "Overall Progress:  20%|██        | 6/30 [00:33<02:13,  5.56s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 20%] ===\n",
      "                        [Original] The old MRT house, home sweet sand drive and trabeca by the waterfront\n",
      "                        [Corrected] the old mrt house, hong huat drive and tribeca by the waterfront.\n",
      "                        [Reference] the old admiralty house hon sui sen drive and tribeca by the waterfronts\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  30%|███       | 9/30 [00:50<01:59,  5.71s/it]\n",
      "Overall Progress:  30%|███       | 9/30 [00:50<01:59,  5.71s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 30%] ===\n",
      "                        [Original] Addu Rakhim Isha, Anthony Chen and Owen Bun-Tek\n",
      "                        [Corrected] addu, rahim, isha, anthony chen and ong boon tek.\n",
      "                        [Reference] abdul rahim ishak anthony chen and ong boon tat\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  40%|████      | 12/30 [01:08<01:43,  5.75s/it]\n",
      "Overall Progress:  40%|████      | 12/30 [01:08<01:43,  5.75s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 40%] ===\n",
      "                        [Original] How much is a cathera?\n",
      "                        [Corrected] how much is a kaya?\n",
      "                        [Reference] how much is air karthira\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  50%|█████     | 15/30 [01:25<01:23,  5.60s/it]\n",
      "Overall Progress:  50%|█████     | 15/30 [01:25<01:23,  5.60s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 50%] ===\n",
      "                        [Original] Lift the BIMI off at 3 Guru Singh Saba\n",
      "                        [Corrected] lift me up at 3 gurdwara singh sabha.\n",
      "                        [Reference] arleth is dropping me off at sri guru singh sabha\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  60%|██████    | 18/30 [01:41<01:05,  5.47s/it]\n",
      "Overall Progress:  60%|██████    | 18/30 [01:41<01:05,  5.47s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 60%] ===\n",
      "                        [Original] 856 41745\n",
      "                        [Corrected] 856-41745\n",
      "                        [Reference] eight five six four one seven four five\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  70%|███████   | 21/30 [01:59<00:51,  5.72s/it]\n",
      "Overall Progress:  70%|███████   | 21/30 [01:59<00:51,  5.72s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 70%] ===\n",
      "                        [Original] John Funs, Nicola, Keok, Ben, Kim, Francis and Elek Keok.\n",
      "                        [Corrected] john fungs, nicole pang, ken francis and ella keough.\n",
      "                        [Reference] john fearns nicoll kwok peng kin francis and alec kuok\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  80%|████████  | 24/30 [02:17<00:36,  6.06s/it]\n",
      "Overall Progress:  80%|████████  | 24/30 [02:17<00:36,  6.06s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 80%] ===\n",
      "                        [Original] Hiyashi chuka taaki komi gohan en fugo.\n",
      "                        [Corrected] hiyashi chuka, takikomi gohan, enough or not?\n",
      "                        [Reference] hiyashi chuka takikomi gohan and fugu\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  90%|█████████ | 27/30 [02:34<00:17,  5.88s/it]\n",
      "Overall Progress:  90%|█████████ | 27/30 [02:34<00:17,  5.88s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 90%] ===\n",
      "                        [Original] Campbell loves chut-papri.\n",
      "                        [Corrected] campbell loves chutney papri.\n",
      "                        [Reference] campbell loves chaat papri\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 30/30 [02:58<00:00,  7.18s/it]\n",
      "Model 1/1: google/gemma-3-27b-i...: 100%|██████████| 30/30 [02:58<00:00,  5.95s/it]\n",
      "Overall Progress: 100%|██████████| 30/30 [02:58<00:00,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 100%] ===\n",
      "                        [Original] Tell me the price of Guizhou.\n",
      "                        [Corrected] tell me the price of guizhou.\n",
      "                        [Reference] tell me the price of kuay chap\n",
      "                        \n",
      "\n",
      "=== Benchmark Complete ===\n",
      "\n",
      "Model: google/gemma-3-27b-it\n",
      "WER Improvement: 0.147\n",
      "Sample Corrections:\n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 10%] ===\n",
      "                        [Original] Castle, Tender, Beef, Raderoo, and Hellmuth.\n",
      "                        [Corrected] cassu, tender beef, radarou and hellmuths.\n",
      "                        [Reference] katsu tendon beef vindaloo and hummus\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 30%] ===\n",
      "                        [Original] Addu Rakhim Isha, Anthony Chen and Owen Bun-Tek\n",
      "                        [Corrected] addu, rahim, isha, anthony chen and ong boon tek.\n",
      "                        [Reference] abdul rahim ishak anthony chen and ong boon tat\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 50%] ===\n",
      "                        [Original] Lift the BIMI off at 3 Guru Singh Saba\n",
      "                        [Corrected] lift me up at 3 gurdwara singh sabha.\n",
      "                        [Reference] arleth is dropping me off at sri guru singh sabha\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 70%] ===\n",
      "                        [Original] John Funs, Nicola, Keok, Ben, Kim, Francis and Elek Keok.\n",
      "                        [Corrected] john fungs, nicole pang, ken francis and ella keough.\n",
      "                        [Reference] john fearns nicoll kwok peng kin francis and alec kuok\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 90%] ===\n",
      "                        [Original] Campbell loves chut-papri.\n",
      "                        [Corrected] campbell loves chutney papri.\n",
      "                        [Reference] campbell loves chaat papri\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM</th>\n",
       "      <th>Original WER</th>\n",
       "      <th>Corrected WER</th>\n",
       "      <th>Improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/gemma-3-27b-it</td>\n",
       "      <td>0.659341</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.14652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     LLM  Original WER  Corrected WER  Improvement\n",
       "0  google/gemma-3-27b-it      0.659341       0.512821      0.14652"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Supported DeepInfra models (check their model zoo):\n",
    "llm_models = [\n",
    "    \"google/gemma-3-27b-it\"\n",
    "\n",
    "]\n",
    "\n",
    "correction_results = benchmark_llm_correction(\n",
    "    \"openai/whisper-small.en\",\n",
    "    llm_models,\n",
    "    num_samples=30\n",
    ")\n",
    "\n",
    "display(correction_results.drop(columns=['Examples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b50ea0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm_models = [\n",
    "#     \"google/gemma-3-27b-it\"\n",
    "# ]\n",
    "\n",
    "# correction_results = benchmark_llm_correction(\n",
    "#     \"mjwong/whisper-small-singlish\",\n",
    "#     llm_models,\n",
    "#     num_samples=30\n",
    "# )\n",
    "\n",
    "# display(correction_results.drop(columns=['Examples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9e2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
