{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8327c380c5fc799",
   "metadata": {},
   "source": [
    "# ASR Benchmarking Notebook for Multitask-National-Speech-Corpus-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9cf495f2ac8268dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.536946Z",
     "start_time": "2025-04-26T15:34:36.798362Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq, WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer, mer, wip, wil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42835e108ea33568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.574726Z",
     "start_time": "2025-04-26T15:34:41.543716Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Configurations -----------------------\n",
    "MODEL_ID = \"openai/whisper-small\"  # Changeable to local or finetuned models\n",
    "DATA_DIR = \"ASR-PART2-Test\"        # Changeable for different partitions\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_SAMPLES = 100                  # Number of samples for quick benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a28e19f9fe22e1a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.665928Z",
     "start_time": "2025-04-26T15:34:41.663219Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Load Dataset -----------------------\n",
    "def load_data(data_dir, split='train', num_samples=None):\n",
    "    dataset = load_dataset('MERaLiON/Multitask-National-Speech-Corpus-v1', data_dir=data_dir)[split]\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ed74e4f5709547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.711229Z",
     "start_time": "2025-04-26T15:34:41.707627Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Load ASR Model -----------------------\n",
    "def load_asr_model(model_id):\n",
    "    try:\n",
    "        # Try using a generic pipeline-based ASR (e.g. Whisper, wav2vec2)\n",
    "        asr_pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model_id,\n",
    "            device=0 if DEVICE == \"cuda\" else -1\n",
    "        )\n",
    "\n",
    "        def wrapped_pipeline(audio_array, sampling_rate=16000):\n",
    "            return asr_pipeline(audio_array)\n",
    "\n",
    "        return wrapped_pipeline\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Falling back to custom processor loading for model {model_id} due to: {e}\")\n",
    "        # Custom processor and model logic\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(DEVICE)\n",
    "\n",
    "        def custom_asr_pipeline(audio_array, sampling_rate=16000):\n",
    "            input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(input_features=input_features)\n",
    "            transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return {\"text\": transcription}\n",
    "\n",
    "        return custom_asr_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9d5f9f24abf45bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.759865Z",
     "start_time": "2025-04-26T15:34:41.755599Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Benchmarking Function -----------------------\n",
    "def benchmark_asr(asr_pipeline, dataset):\n",
    "    results = []\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Transcribing\"):\n",
    "        audio_array = example[\"context\"][\"array\"]\n",
    "        reference = example[\"answer\"]\n",
    "\n",
    "        start = time.time()\n",
    "        prediction = asr_pipeline(audio_array, sampling_rate=16000)[\"text\"]\n",
    "        end = time.time()\n",
    "\n",
    "        results.append({\n",
    "            \"reference\": reference,\n",
    "            \"prediction\": prediction,\n",
    "            \"time\": end - start\n",
    "        })\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d9b27ffa129f35e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.812206Z",
     "start_time": "2025-04-26T15:34:41.806955Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Evaluation Metrics -----------------------\n",
    "def evaluate_metrics(result_df):\n",
    "    result_df[\"wer\"] = result_df.apply(lambda x: wer(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"mer\"] = result_df.apply(lambda x: mer(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"wil\"] = result_df.apply(lambda x: wil(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"wip\"] = result_df.apply(lambda x: wip(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "\n",
    "    average_metrics = {\n",
    "        \"Average WER\": result_df[\"wer\"].mean(),\n",
    "        \"Average MER\": result_df[\"mer\"].mean(),\n",
    "        \"Average WIL\": result_df[\"wil\"].mean(),\n",
    "        \"Average WIP\": result_df[\"wip\"].mean(),\n",
    "        \"Average Time per Sample (s)\": result_df[\"time\"].mean()\n",
    "    }\n",
    "\n",
    "    return result_df, average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "476ddff3520b996b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.860038Z",
     "start_time": "2025-04-26T15:34:41.855565Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Identify Best/Worst Words -----------------------\n",
    "def word_analysis(result_df):\n",
    "    from collections import Counter\n",
    "    word_errors = []\n",
    "    for _, row in result_df.iterrows():\n",
    "        ref_words = row[\"reference\"].lower().split()\n",
    "        pred_words = row[\"prediction\"].lower().split()\n",
    "        missed = set(ref_words) - set(pred_words)\n",
    "        for word in missed:\n",
    "            word_errors.append(word)\n",
    "\n",
    "    error_counter = Counter(word_errors)\n",
    "    most_common_errors = error_counter.most_common(10)\n",
    "    least_common_errors = error_counter.most_common()[-10:]\n",
    "    return most_common_errors, least_common_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1e7e36a60f042748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.915463Z",
     "start_time": "2025-04-26T15:34:41.904208Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from jiwer import wer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the reference text by:\n",
    "    - Removing speaker labels like <speaker1>: or <speaker2>:\n",
    "    - Removing round or square brackets but keeping the content inside.\n",
    "    - Converting to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove speaker tags like <speaker1>: or <speaker2>:\n",
    "    text = re.sub(r\"<speaker\\d+>:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove round or square brackets but keep content inside\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\((.*?)\\)\", r\"\\1\", text)\n",
    "\n",
    "    # Convert to lowercase and strip surrounding whitespace\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def run_benchmark(model_id, data_dir=\"ASR-PART1-Train\", num_samples=100, batch_size=8):\n",
    "    print(f\"\\n📊 Running benchmark for model: {model_id}\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"MERaLiON/Multitask-National-Speech-Corpus-v1\", data_dir=data_dir)[\"train\"]\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    # Load ASR model pipeline\n",
    "    model_fn = load_asr_model(model_id)\n",
    "\n",
    "    # Prepare audio and references\n",
    "    audio_arrays = [sample[\"context\"][\"array\"] for sample in dataset]\n",
    "    references = [sample[\"answer\"] for sample in dataset]\n",
    "\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run batched inference\n",
    "    for i in tqdm(range(0, len(audio_arrays), batch_size), desc=f\"Evaluating {model_id}\", ncols=100):\n",
    "        batch_audio = audio_arrays[i:i+batch_size]\n",
    "        batch_refs = references[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            # Support both batch and single-audio pipelines\n",
    "            preds = model_fn(batch_audio)\n",
    "\n",
    "            # Ensure output is iterable\n",
    "            if isinstance(preds, dict):\n",
    "                preds = [preds]\n",
    "\n",
    "            for ref, pred_dict in zip(batch_refs, preds):\n",
    "                hyp = pred_dict.get(\"text\", \"[ERROR]\").strip()\n",
    "\n",
    "                # Clean both reference and prediction\n",
    "                ref_cleaned = clean_text(ref)\n",
    "                hyp_cleaned = clean_text(hyp)\n",
    "\n",
    "                predictions.append({\"reference\": ref_cleaned, \"prediction\": hyp_cleaned})\n",
    "\n",
    "        except Exception as e:\n",
    "            for ref in batch_refs:\n",
    "                predictions.append({\"reference\": clean_text(ref), \"prediction\": f\"[ERROR: {e}]\"})\n",
    "\n",
    "        # Dynamically print progress without breaking the progress bar\n",
    "        tqdm.write(f\"Processed {i + batch_size}/{len(audio_arrays)} samples.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Compute metrics\n",
    "    references_clean = [x[\"reference\"] for x in predictions]\n",
    "    predictions_clean = [x[\"prediction\"] for x in predictions]\n",
    "    computed_wer = wer(references_clean, predictions_clean)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n✅ Benchmark complete for {model_id}\")\n",
    "    print(f\"⏱️ Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"🧠 WER: {computed_wer:.4f}\")\n",
    "\n",
    "    # Sample output\n",
    "    print(\"\\n🔍 Sample predictions:\")\n",
    "    for sample in predictions[:5]:\n",
    "        print(f\"REF: {sample['reference']}\")\n",
    "        print(f\"HYP: {sample['prediction']}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"wer\": computed_wer,\n",
    "        \"runtime_sec\": total_time,\n",
    "        \"results\": predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f7dc245ce2e555c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:44:01.255986Z",
     "start_time": "2025-04-26T15:34:43.420731Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Multi-Model Benchmark Loop -----------------------\n",
    "import gc\n",
    "\n",
    "def benchmark_multiple_models(model_ids, data_dir=\"ASR-PART5-Test\", num_samples=100, batch_size=8):\n",
    "    comparison_results = []\n",
    "\n",
    "    for model_id in model_ids:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() # Clear the GPU cache\n",
    "\n",
    "        print(f\"\\n===== Benchmarking Model: {model_id} =====\")\n",
    "\n",
    "        # Run benchmark and get results\n",
    "        result = run_benchmark(model_id=model_id, data_dir=data_dir, num_samples=num_samples, batch_size=batch_size)\n",
    "\n",
    "        # Collecting metrics for comparison\n",
    "        metrics = {\"WER\": result[\"wer\"], \"Runtime (sec)\": result[\"runtime_sec\"], \"Model\": model_id}\n",
    "        comparison_results.append(metrics)\n",
    "\n",
    "    # Convert list of dicts to DataFrame for easy comparison\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    comparison_df = comparison_df.set_index(\"Model\")\n",
    "\n",
    "    return comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98df652accba471",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEPINFRA_API_KEY = \"API KEY\"  \n",
    "DEEPINFRA_API_BASE = \"https://api.deepinfra.com/v1/openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d5791553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_with_llm(hypotheses, llm_type=\"meta-llama/Llama-3-70b-chat-hf\"):\n",
    "    \"\"\"Use DeepInfra-hosted LLM for correction\"\"\"\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=DEEPINFRA_API_KEY,\n",
    "        base_url=DEEPINFRA_API_BASE\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"Correct this speech transcription, which is in Singlish and likely to contain words related to Singapore. You may need to correct it based on the knowledge you have about Singapore, but you shall correct it only when it's necessary. Using these ASR hypotheses and their confidence scores:\n",
    "    \n",
    "    {chr(10).join([f'- {h[\"text\"]} (confidence: {h[\"confidence\"]:.2f})' for h in hypotheses])}\n",
    "    \n",
    "    Do not include any other text or explanation. Just provide the corrected transcript.\"\"\"\n",
    "    #\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_type,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def generate_whisper_hypotheses(\n",
    "    audio_array,\n",
    "    model_id=\"openai/whisper-small.en\",\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    sampling_rate=16000,\n",
    "    num_hypotheses=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate Whisper ASR hypotheses with one confidence score each.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: [{\"text\": str, \"confidence\": float}, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "    processor = WhisperProcessor.from_pretrained(model_id)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "    features = inputs.input_features.to(device)\n",
    "\n",
    "    #Generate N-best sequences with scores\n",
    "    outputs = model.generate(\n",
    "        features,\n",
    "        do_sample=True,            \n",
    "        num_return_sequences=3,    \n",
    "        max_new_tokens=256,\n",
    "        top_k=50,                  \n",
    "        top_p=0.95,                \n",
    "        temperature=0.7,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    hypotheses = []\n",
    "    for idx in range(num_hypotheses):\n",
    "        seq_ids = outputs.sequences[idx]\n",
    "        text = processor.decode(seq_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "        if hasattr(outputs, \"sequences_scores\") and outputs.sequences_scores is not None:\n",
    "            sum_logprob = outputs.sequences_scores[idx]\n",
    "        else:\n",
    "\n",
    "            sum_logprob = 0.0\n",
    "\n",
    "            for step, step_scores in enumerate(outputs.scores):\n",
    "                logprobs = torch.log_softmax(step_scores, dim=-1)\n",
    "                token_id = seq_ids[step + 1]  # skip <s> token\n",
    "\n",
    "                sum_logprob += logprobs[idx, token_id]\n",
    "\n",
    "        token_count = seq_ids.shape[-1] - 1\n",
    "        avg_logprob = (sum_logprob / token_count).item()\n",
    "\n",
    "        hypotheses.append({\"text\": text, \"confidence\": avg_logprob})\n",
    "\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Enhanced Benchmarking with Progress -----------------------\n",
    "def benchmark_llm_correction(asr_model_id, llm_models, num_samples=50):\n",
    "    dataset = load_data(DATA_DIR, num_samples=num_samples)\n",
    "\n",
    "    \n",
    "    results = []\n",
    "    total_models = len(llm_models)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    with tqdm(total=total_models * total_samples, desc=\"Overall Progress\") as main_pbar:\n",
    "        for llm_idx, llm in enumerate(llm_models):\n",
    "            model_progress = {\n",
    "                \"processed\": 0,\n",
    "                \"examples\": []\n",
    "            }\n",
    "            \n",
    "            with tqdm(dataset, desc=f\"Model {llm_idx+1}/{total_models}: {llm[:20]}...\") as model_pbar:\n",
    "                corrected = []\n",
    "                for sample_idx, example in enumerate(dataset):\n",
    "                    # Generate ASR hypotheses\n",
    "                    hypotheses = generate_whisper_hypotheses(example[\"context\"][\"array\"], model_id=asr_model_id)\n",
    "                    # LLM Correction\n",
    "                    corrected_text = correct_with_llm(hypotheses, llm)\n",
    "                    \n",
    "                    # Store results\n",
    "                    corrected.append({\n",
    "                        \"original\": hypotheses[0][\"text\"],\n",
    "                        \"corrected\": clean_text(corrected_text),\n",
    "                        \"reference\": clean_text(example[\"answer\"])\n",
    "                    })\n",
    "                    \n",
    "                    model_progress[\"processed\"] += 1\n",
    "                    main_pbar.update(1)\n",
    "                    model_pbar.update(1)\n",
    "                    \n",
    "                    # Show examples every 10% of samples\n",
    "                    if (sample_idx + 1) % max(1, total_samples//10) == 0:\n",
    "                        progress_percent = int(100 * (sample_idx + 1) / total_samples)\n",
    "                        latest_example = corrected[-1]\n",
    "                        \n",
    "                        example_output = f\"\"\"\n",
    "                        \\n=== [Model: {llm} | Progress: {progress_percent}%] ===\n",
    "                        [Original] {latest_example['original']}\n",
    "                        [Corrected] {latest_example['corrected']}\n",
    "                        [Reference] {latest_example['reference']}\n",
    "                        \"\"\"\n",
    "                        tqdm.write(example_output)\n",
    "                        model_progress[\"examples\"].append(example_output)\n",
    "\n",
    "                # Calculate metrics\n",
    "                wer_original = wer([c[\"reference\"] for c in corrected], [c[\"original\"] for c in corrected])\n",
    "                wer_corrected = wer([c[\"reference\"] for c in corrected], [c[\"corrected\"] for c in corrected])\n",
    "                \n",
    "                results.append({\n",
    "                    \"LLM\": llm,\n",
    "                    \"Original WER\": wer_original,\n",
    "                    \"Corrected WER\": wer_corrected,\n",
    "                    \"Improvement\": wer_original - wer_corrected,\n",
    "                    \"Examples\": model_progress[\"examples\"]\n",
    "                })\n",
    "\n",
    "    # Final output with examples\n",
    "    print(\"\\n=== Benchmark Complete ===\")\n",
    "    for result in results:\n",
    "        print(f\"\\nModel: {result['LLM']}\")\n",
    "        print(f\"WER Improvement: {result['Improvement']:.3f}\")\n",
    "        print(\"Sample Corrections:\")\n",
    "        for example in result['Examples'][::2]:  \n",
    "            print(example)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9f93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/30 [00:00<?, ?it/s]d:\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_scores` is. When `return_dict_in_generate` is not `True`, `output_scores` is ignored.\n",
      "  warnings.warn(\n",
      "Overall Progress:  10%|█         | 3/30 [00:17<02:33,  5.70s/it]\n",
      "Overall Progress:  10%|█         | 3/30 [00:17<02:33,  5.70s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 10%] ===\n",
      "                        [Original] Castle, Tender, Beef, Raderoo, and Hellmuth.\n",
      "                        [Corrected] cassu, tender beef, radarou and hellmuths.\n",
      "                        [Reference] katsu tendon beef vindaloo and hummus\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  20%|██        | 6/30 [00:33<02:13,  5.56s/it]\n",
      "Overall Progress:  20%|██        | 6/30 [00:33<02:13,  5.56s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 20%] ===\n",
      "                        [Original] The old MRT house, home sweet sand drive and trabeca by the waterfront\n",
      "                        [Corrected] the old mrt house, hong huat drive and tribeca by the waterfront.\n",
      "                        [Reference] the old admiralty house hon sui sen drive and tribeca by the waterfronts\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  30%|███       | 9/30 [00:50<01:59,  5.71s/it]\n",
      "Overall Progress:  30%|███       | 9/30 [00:50<01:59,  5.71s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 30%] ===\n",
      "                        [Original] Addu Rakhim Isha, Anthony Chen and Owen Bun-Tek\n",
      "                        [Corrected] addu, rahim, isha, anthony chen and ong boon tek.\n",
      "                        [Reference] abdul rahim ishak anthony chen and ong boon tat\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  40%|████      | 12/30 [01:08<01:43,  5.75s/it]\n",
      "Overall Progress:  40%|████      | 12/30 [01:08<01:43,  5.75s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 40%] ===\n",
      "                        [Original] How much is a cathera?\n",
      "                        [Corrected] how much is a kaya?\n",
      "                        [Reference] how much is air karthira\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  50%|█████     | 15/30 [01:25<01:23,  5.60s/it]\n",
      "Overall Progress:  50%|█████     | 15/30 [01:25<01:23,  5.60s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 50%] ===\n",
      "                        [Original] Lift the BIMI off at 3 Guru Singh Saba\n",
      "                        [Corrected] lift me up at 3 gurdwara singh sabha.\n",
      "                        [Reference] arleth is dropping me off at sri guru singh sabha\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  60%|██████    | 18/30 [01:41<01:05,  5.47s/it]\n",
      "Overall Progress:  60%|██████    | 18/30 [01:41<01:05,  5.47s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 60%] ===\n",
      "                        [Original] 856 41745\n",
      "                        [Corrected] 856-41745\n",
      "                        [Reference] eight five six four one seven four five\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  70%|███████   | 21/30 [01:59<00:51,  5.72s/it]\n",
      "Overall Progress:  70%|███████   | 21/30 [01:59<00:51,  5.72s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 70%] ===\n",
      "                        [Original] John Funs, Nicola, Keok, Ben, Kim, Francis and Elek Keok.\n",
      "                        [Corrected] john fungs, nicole pang, ken francis and ella keough.\n",
      "                        [Reference] john fearns nicoll kwok peng kin francis and alec kuok\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  80%|████████  | 24/30 [02:17<00:36,  6.06s/it]\n",
      "Overall Progress:  80%|████████  | 24/30 [02:17<00:36,  6.06s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 80%] ===\n",
      "                        [Original] Hiyashi chuka taaki komi gohan en fugo.\n",
      "                        [Corrected] hiyashi chuka, takikomi gohan, enough or not?\n",
      "                        [Reference] hiyashi chuka takikomi gohan and fugu\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  90%|█████████ | 27/30 [02:34<00:17,  5.88s/it]\n",
      "Overall Progress:  90%|█████████ | 27/30 [02:34<00:17,  5.88s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 90%] ===\n",
      "                        [Original] Campbell loves chut-papri.\n",
      "                        [Corrected] campbell loves chutney papri.\n",
      "                        [Reference] campbell loves chaat papri\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 30/30 [02:58<00:00,  7.18s/it]\n",
      "Model 1/1: google/gemma-3-27b-i...: 100%|██████████| 30/30 [02:58<00:00,  5.95s/it]\n",
      "Overall Progress: 100%|██████████| 30/30 [02:58<00:00,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 100%] ===\n",
      "                        [Original] Tell me the price of Guizhou.\n",
      "                        [Corrected] tell me the price of guizhou.\n",
      "                        [Reference] tell me the price of kuay chap\n",
      "                        \n",
      "\n",
      "=== Benchmark Complete ===\n",
      "\n",
      "Model: google/gemma-3-27b-it\n",
      "WER Improvement: 0.147\n",
      "Sample Corrections:\n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 10%] ===\n",
      "                        [Original] Castle, Tender, Beef, Raderoo, and Hellmuth.\n",
      "                        [Corrected] cassu, tender beef, radarou and hellmuths.\n",
      "                        [Reference] katsu tendon beef vindaloo and hummus\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 30%] ===\n",
      "                        [Original] Addu Rakhim Isha, Anthony Chen and Owen Bun-Tek\n",
      "                        [Corrected] addu, rahim, isha, anthony chen and ong boon tek.\n",
      "                        [Reference] abdul rahim ishak anthony chen and ong boon tat\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 50%] ===\n",
      "                        [Original] Lift the BIMI off at 3 Guru Singh Saba\n",
      "                        [Corrected] lift me up at 3 gurdwara singh sabha.\n",
      "                        [Reference] arleth is dropping me off at sri guru singh sabha\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 70%] ===\n",
      "                        [Original] John Funs, Nicola, Keok, Ben, Kim, Francis and Elek Keok.\n",
      "                        [Corrected] john fungs, nicole pang, ken francis and ella keough.\n",
      "                        [Reference] john fearns nicoll kwok peng kin francis and alec kuok\n",
      "                        \n",
      "\n",
      "                        \n",
      "=== [Model: google/gemma-3-27b-it | Progress: 90%] ===\n",
      "                        [Original] Campbell loves chut-papri.\n",
      "                        [Corrected] campbell loves chutney papri.\n",
      "                        [Reference] campbell loves chaat papri\n",
      "                        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM</th>\n",
       "      <th>Original WER</th>\n",
       "      <th>Corrected WER</th>\n",
       "      <th>Improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/gemma-3-27b-it</td>\n",
       "      <td>0.659341</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.14652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     LLM  Original WER  Corrected WER  Improvement\n",
       "0  google/gemma-3-27b-it      0.659341       0.512821      0.14652"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "llm_models = [\n",
    "    \"google/gemma-3-27b-it\"\n",
    "\n",
    "]\n",
    "\n",
    "correction_results = benchmark_llm_correction(\n",
    "    \"openai/whisper-small.en\",\n",
    "    llm_models,\n",
    "    num_samples=30\n",
    ")\n",
    "\n",
    "display(correction_results.drop(columns=['Examples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b50ea0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm_models = [\n",
    "#     \"google/gemma-3-27b-it\"\n",
    "# ]\n",
    "\n",
    "# correction_results = benchmark_llm_correction(\n",
    "#     \"mjwong/whisper-small-singlish\",\n",
    "#     llm_models,\n",
    "#     num_samples=30\n",
    "# )\n",
    "\n",
    "# display(correction_results.drop(columns=['Examples']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
