{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8327c380c5fc799",
   "metadata": {},
   "source": [
    "# ASR Benchmarking Notebook for Multitask-National-Speech-Corpus-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf495f2ac8268dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.536946Z",
     "start_time": "2025-04-26T15:34:36.798362Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq, WhisperProcessor, WhisperForConditionalGeneration\n",
    "from jiwer import wer, mer, wip, wil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42835e108ea33568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.574726Z",
     "start_time": "2025-04-26T15:34:41.543716Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Configurations -----------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed74e4f5709547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.711229Z",
     "start_time": "2025-04-26T15:34:41.707627Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Load ASR Model -----------------------\n",
    "def load_asr_model(model_id):\n",
    "    try:\n",
    "        # Try using a generic pipeline-based ASR (e.g. Whisper, wav2vec2)\n",
    "        asr_pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model_id,\n",
    "            device=0 if DEVICE == \"cuda\" else -1\n",
    "        )\n",
    "\n",
    "        def wrapped_pipeline(audio_array, sampling_rate=16000):\n",
    "            return asr_pipeline(audio_array)\n",
    "\n",
    "        return wrapped_pipeline\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Falling back to custom processor loading for model {model_id} due to: {e}\")\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(DEVICE)\n",
    "\n",
    "        def custom_asr_pipeline(audio_array, sampling_rate=16000):\n",
    "            input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(input_features=input_features)\n",
    "            transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return {\"text\": transcription}\n",
    "\n",
    "        return custom_asr_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5f9f24abf45bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.759865Z",
     "start_time": "2025-04-26T15:34:41.755599Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Benchmarking Function -----------------------\n",
    "def benchmark_asr(asr_pipeline, dataset):\n",
    "    results = []\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    for example in tqdm(dataset, desc=\"Transcribing\"):\n",
    "        audio_array = example[\"context\"][\"array\"]\n",
    "        reference = example[\"answer\"]\n",
    "\n",
    "        start = time.time()\n",
    "        prediction = asr_pipeline(audio_array, sampling_rate=16000)[\"text\"]\n",
    "        end = time.time()\n",
    "\n",
    "        results.append({\n",
    "            \"reference\": reference,\n",
    "            \"prediction\": prediction,\n",
    "            \"time\": end - start\n",
    "        })\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b27ffa129f35e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.812206Z",
     "start_time": "2025-04-26T15:34:41.806955Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Evaluation Metrics -----------------------\n",
    "def evaluate_metrics(result_df):\n",
    "    result_df[\"wer\"] = result_df.apply(lambda x: wer(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"mer\"] = result_df.apply(lambda x: mer(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"wil\"] = result_df.apply(lambda x: wil(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "    result_df[\"wip\"] = result_df.apply(lambda x: wip(x[\"reference\"], x[\"prediction\"]), axis=1)\n",
    "\n",
    "    average_metrics = {\n",
    "        \"Average WER\": result_df[\"wer\"].mean(),\n",
    "        \"Average MER\": result_df[\"mer\"].mean(),\n",
    "        \"Average WIL\": result_df[\"wil\"].mean(),\n",
    "        \"Average WIP\": result_df[\"wip\"].mean(),\n",
    "        \"Average Time per Sample (s)\": result_df[\"time\"].mean()\n",
    "    }\n",
    "\n",
    "    return result_df, average_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476ddff3520b996b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.860038Z",
     "start_time": "2025-04-26T15:34:41.855565Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Identify Best/Worst Words -----------------------\n",
    "def word_analysis(result_df):\n",
    "    from collections import Counter\n",
    "    word_errors = []\n",
    "    for _, row in result_df.iterrows():\n",
    "        ref_words = row[\"reference\"].lower().split()\n",
    "        pred_words = row[\"prediction\"].lower().split()\n",
    "        missed = set(ref_words) - set(pred_words)\n",
    "        for word in missed:\n",
    "            word_errors.append(word)\n",
    "\n",
    "    error_counter = Counter(word_errors)\n",
    "    most_common_errors = error_counter.most_common(10)\n",
    "    least_common_errors = error_counter.most_common()[-10:]\n",
    "    return most_common_errors, least_common_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7e36a60f042748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:34:41.915463Z",
     "start_time": "2025-04-26T15:34:41.904208Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from jiwer import wer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the reference text by:\n",
    "    - Removing speaker labels like <speaker1>: or <speaker2>:\n",
    "    - Removing round or square brackets but keeping the content inside.\n",
    "    - Converting to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove speaker tags like <speaker1>: or <speaker2>:\n",
    "    text = re.sub(r\"<speaker\\d+>:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove round or square brackets but keep content inside\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\((.*?)\\)\", r\"\\1\", text)\n",
    "\n",
    "    # Convert to lowercase and strip surrounding whitespace\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "def run_benchmark(model_id, data_dir=\"ASR-PART1-Train\", num_samples=100, batch_size=8):\n",
    "    print(f\"\\n📊 Running benchmark for model: {model_id}\")\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"MERaLiON/Multitask-National-Speech-Corpus-v1\", data_dir=data_dir)[\"train\"]\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    # Load ASR model pipeline\n",
    "    model_fn = load_asr_model(model_id)\n",
    "\n",
    "    # Prepare audio and references\n",
    "    audio_arrays = [sample[\"context\"][\"array\"] for sample in dataset]\n",
    "    references = [sample[\"answer\"] for sample in dataset]\n",
    "\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run batched inference\n",
    "    for i in tqdm(range(0, len(audio_arrays), batch_size), desc=f\"Evaluating {model_id}\", ncols=100):\n",
    "        batch_audio = audio_arrays[i:i+batch_size]\n",
    "        batch_refs = references[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            # Support both batch and single-audio pipelines\n",
    "            preds = model_fn(batch_audio)\n",
    "\n",
    "            # Ensure output is iterable\n",
    "            if isinstance(preds, dict):\n",
    "                preds = [preds]\n",
    "\n",
    "            for ref, pred_dict in zip(batch_refs, preds):\n",
    "                hyp = pred_dict.get(\"text\", \"[ERROR]\").strip()\n",
    "\n",
    "                # Clean both reference and prediction\n",
    "                ref_cleaned = clean_text(ref)\n",
    "                hyp_cleaned = clean_text(hyp)\n",
    "\n",
    "                predictions.append({\"reference\": ref_cleaned, \"prediction\": hyp_cleaned})\n",
    "\n",
    "        except Exception as e:\n",
    "            for ref in batch_refs:\n",
    "                predictions.append({\"reference\": clean_text(ref), \"prediction\": f\"[ERROR: {e}]\"})\n",
    "\n",
    "        # Dynamically print progress without breaking the progress bar\n",
    "        tqdm.write(f\"Processed {i + batch_size}/{len(audio_arrays)} samples.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    # Compute metrics\n",
    "    references_clean = [x[\"reference\"] for x in predictions]\n",
    "    predictions_clean = [x[\"prediction\"] for x in predictions]\n",
    "    computed_wer = wer(references_clean, predictions_clean)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n✅ Benchmark complete for {model_id}\")\n",
    "    print(f\"⏱️ Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"🧠 WER: {computed_wer:.4f}\")\n",
    "\n",
    "    # Sample output\n",
    "    print(\"\\n🔍 Sample predictions:\")\n",
    "    for sample in predictions[:5]:\n",
    "        print(f\"REF: {sample['reference']}\")\n",
    "        print(f\"HYP: {sample['prediction']}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"wer\": computed_wer,\n",
    "        \"runtime_sec\": total_time,\n",
    "        \"results\": predictions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7dc245ce2e555c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T15:44:01.255986Z",
     "start_time": "2025-04-26T15:34:43.420731Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------- Multi-Model Benchmark Loop -----------------------\n",
    "import gc\n",
    "\n",
    "def benchmark_multiple_models(model_ids, data_dir=\"ASR-PART6-Test\", num_samples=100, batch_size=8):\n",
    "    comparison_results = []\n",
    "\n",
    "    for model_id in model_ids:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() # Clear the GPU cache\n",
    "\n",
    "        print(f\"\\n===== Benchmarking Model: {model_id} =====\")\n",
    "\n",
    "        # Run benchmark and get results\n",
    "        result = run_benchmark(model_id=model_id, data_dir=data_dir, num_samples=num_samples, batch_size=batch_size)\n",
    "\n",
    "        # Collecting metrics for comparison\n",
    "        metrics = {\"WER\": result[\"wer\"], \"Runtime (sec)\": result[\"runtime_sec\"], \"Model\": model_id}\n",
    "        comparison_results.append(metrics)\n",
    "\n",
    "    # Convert list of dicts to DataFrame for easy comparison\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    comparison_df = comparison_df.set_index(\"Model\")\n",
    "\n",
    "    return comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0bf4a4b533b3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Benchmarking Model: openai/whisper-small.en =====\n",
      "\n",
      "📊 Running benchmark for model: openai/whisper-small.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Evaluating openai/whisper-small.en:   0%|                                    | 0/13 [00:00<?, ?it/s]C:\\Users\\tan_l\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Evaluating openai/whisper-small.en:   8%|██▏                         | 1/13 [00:05<01:05,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  15%|████▎                       | 2/13 [00:10<00:54,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  23%|██████▍                     | 3/13 [00:14<00:45,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  31%|████████▌                   | 4/13 [00:19<00:42,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  38%|██████████▊                 | 5/13 [00:22<00:35,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  46%|████████████▉               | 6/13 [00:27<00:31,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 48/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  54%|███████████████             | 7/13 [00:31<00:26,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  62%|█████████████████▏          | 8/13 [00:36<00:21,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 64/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  69%|███████████████████▍        | 9/13 [00:40<00:17,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 72/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  77%|████████████████████▊      | 10/13 [00:44<00:13,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  85%|██████████████████████▊    | 11/13 [00:49<00:08,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 88/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en:  92%|████████████████████████▉  | 12/13 [00:54<00:04,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 96/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating openai/whisper-small.en: 100%|███████████████████████████| 13/13 [00:55<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 104/100 samples.\n",
      "\n",
      "✅ Benchmark complete for openai/whisper-small.en\n",
      "⏱️ Total time: 56.00 seconds\n",
      "🧠 WER: 0.3396\n",
      "\n",
      "🔍 Sample predictions:\n",
      "REF: so it depends on the child as compared to jc itself you are basically the child would be you know studying throughout the two whole two years to prepare for one whole major exam which which is the a levels\n",
      "ah okay um so i actually like to inquire more about the cca in polytechnic and also jc um\n",
      "HYP: so it depends on the child. as compared to jc itself, basically the child would be studying throughout the whole two years to prepare for one whole major exam which is the a levels. so i'd actually like to inquire more about the cca in polytania, so jcs.\n",
      "-----\n",
      "REF: uh currently my parents they are fifty seven both fifty seven this year so uh i think both of them are working desk bound jobs ya the normal nine to five jobs and ya i don't think there's any risk in terms of health wise ya i would say they're rather healthy\n",
      "HYP: currently my parents, they are 57, both 57 this year. so i think both of them are working desk-bound jobs. they are the normal 95 jobs and i don't think there's any risk. in terms of health-wise, i would say they are rather healthy.\n",
      "-----\n",
      "REF: okay can so uh you don't have to apply just as long as you register your baby during the birth certificate period right then the government knows that there's a new born child that you have um um delivered then afterwards um the payment right would not be one whole shot so it will basically um be periodically so\n",
      "HYP: okay, so you don't have to apply just as long as you register your baby during the birth certificate period, right? then the government knows that there's a newborn child that you have delivered. then afterwards, the payment would not be one whole shot. so it would basically be periodically.\n",
      "-----\n",
      "REF: uh the pets will also you will be able to claim for your pets as well so um okay two of your cats so anyway take note that we will have to do err health checkup first before they can err you are able to purchase this insurance travel insurance for them is it okay with you\n",
      "oh okay so for the uh health checkup um how long in advance uh would i have to take my pets to do the checkup and um any idea what are the cost\n",
      "HYP: the pets will also be able to claim for your pets as well. so, okay, two of your cats. so anyway take note that we will have to do a health checkup first before they can, you are able to purchase this insurance, travel insurance for them. is it okay with you? okay, so for the health checkup, how long in advance would i have to take my pets to do the checkup and any other other cause?\n",
      "-----\n",
      "REF: okay yup you ha~ uh did apply for our you did apply for our premium plan so i think it'll actually be covered but uh okay uh do you have any evidence of you converting the money to korean won\n",
      "sorry again\n",
      "do you have any evidence of you converting the singapore cash to korean won so we can gauge the value that were used to convert to the korean currency\n",
      "i'm not sure but um\n",
      "HYP: okay, yeah, you did apply for our you did apply for our premium plan, so i think you actually be covered but okay, yeah any evidence of you converting the money to korean won sorry again do you have any evidence of you converting the singapore cash to korean won so we can get your value that you use to convert to the current currency? i'm not sure but\n",
      "-----\n",
      "\n",
      "===== Benchmarking Model: jensenlwt/whisper-small-singlish-122k =====\n",
      "\n",
      "📊 Running benchmark for model: jensenlwt/whisper-small-singlish-122k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Evaluating jensenlwt/whisper-small-singlish-122k:   8%|█             | 1/13 [00:05<01:02,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  15%|██▏           | 2/13 [00:13<01:15,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  23%|███▏          | 3/13 [00:21<01:14,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  31%|████▎         | 4/13 [00:27<01:01,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  38%|█████▍        | 5/13 [00:38<01:06,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  46%|██████▍       | 6/13 [00:43<00:51,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 48/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  54%|███████▌      | 7/13 [00:51<00:45,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  62%|████████▌     | 8/13 [00:57<00:34,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 64/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  69%|█████████▋    | 9/13 [01:03<00:26,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 72/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  77%|██████████   | 10/13 [01:11<00:21,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  85%|███████████  | 11/13 [01:16<00:13,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 88/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k:  92%|████████████ | 12/13 [01:22<00:06,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 96/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating jensenlwt/whisper-small-singlish-122k: 100%|█████████████| 13/13 [01:24<00:00,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 104/100 samples.\n",
      "\n",
      "✅ Benchmark complete for jensenlwt/whisper-small-singlish-122k\n",
      "⏱️ Total time: 84.28 seconds\n",
      "🧠 WER: 0.5954\n",
      "\n",
      "🔍 Sample predictions:\n",
      "REF: so it depends on the child as compared to jc itself you are basically the child would be you know studying throughout the two whole two years to prepare for one whole major exam which which is the a levels\n",
      "ah okay um so i actually like to inquire more about the cca in polytechnic and also jc um\n",
      "HYP: so it depends on the child and as come back to j c its self you are basically the child would be you know studied throughout the two whole two years to prepare for one whole major exam which is the a levels okay so i'd actually like to enquire more about the c c a's in polytechnic and also j c's\n",
      "-----\n",
      "REF: uh currently my parents they are fifty seven both fifty seven this year so uh i think both of them are working desk bound jobs ya the normal nine to five jobs and ya i don't think there's any risk in terms of health wise ya i would say they're rather healthy\n",
      "HYP: err currently my parents their fifty seven both fifty seven this year so a i think both of them are working desmond jobs the normal ninety five jobs and i don't think there's any risk in terms of health wise i would say they are rather healthy\n",
      "-----\n",
      "REF: okay can so uh you don't have to apply just as long as you register your baby during the birth certificate period right then the government knows that there's a new born child that you have um um delivered then afterwards um the payment right would not be one whole shot so it will basically um be periodically so\n",
      "HYP: okay ken so you don't have to apply just as long as you register your baby during the birth certificate ** right then the government knows that there's a new born child that you have a a delivered then afterwards the payment right would not be one whole shot so it will basically be periodically so\n",
      "-----\n",
      "REF: uh the pets will also you will be able to claim for your pets as well so um okay two of your cats so anyway take note that we will have to do err health checkup first before they can err you are able to purchase this insurance travel insurance for them is it okay with you\n",
      "oh okay so for the uh health checkup um how long in advance uh would i have to take my pets to do the checkup and um any idea what are the cost\n",
      "HYP: the pets you'll be able to claim for your pets as well so two of your pets so take note that we will have to do a half checkup first before they can you are able to purchase this and travel insurance for them is it okay with you? okay so for the health checkup how long in advance would i have to take my pets to do the checkup and any i did what are the costs\n",
      "-----\n",
      "REF: okay yup you ha~ uh did apply for our you did apply for our premium plan so i think it'll actually be covered but uh okay uh do you have any evidence of you converting the money to korean won\n",
      "sorry again\n",
      "do you have any evidence of you converting the singapore cash to korean won so we can gauge the value that were used to convert to the korean currency\n",
      "i'm not sure but um\n",
      "HYP: okay yep you did a buy for our you did a buy for our premium plan so i think you are actually be covered but a okay a do you have any evidence of you converting the money to korean won sorry again do you have any evidence of you converting the singapore cash to korean won so we can gauge the value that we use to convert to the korean currency i'm not sure about\n",
      "-----\n",
      "\n",
      "===== Benchmarking Model: mjwong/whisper-large-v3-turbo-singlish =====\n",
      "\n",
      "📊 Running benchmark for model: mjwong/whisper-large-v3-turbo-singlish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:   8%|▋        | 1/13 [30:21<6:04:15, 1821.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  15%|█▌        | 2/13 [34:29<2:44:14, 895.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  23%|██▎       | 3/13 [38:34<1:39:46, 598.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 24/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  31%|███       | 4/13 [42:48<1:09:24, 462.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 32/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  38%|████▌       | 5/13 [46:51<51:07, 383.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  46%|█████▌      | 6/13 [51:02<39:29, 338.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 48/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  54%|██████▍     | 7/13 [55:10<30:52, 308.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  62%|███████▍    | 8/13 [59:14<24:01, 288.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 64/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  69%|██████▉   | 9/13 [1:03:25<18:25, 276.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 72/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  77%|██████▉  | 10/13 [1:07:29<13:19, 266.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  85%|███████▌ | 11/13 [1:11:39<08:42, 261.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 88/100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating mjwong/whisper-large-v3-turbo-singlish:  85%|███████▌ | 11/13 [1:36:05<17:28, 524.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m      2\u001b[39m model_list = [\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# \"openai/whisper-medium.en\",\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mopenai/whisper-small.en\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# \"path/to/your/local/model\",  # Example for a local model\u001b[39;00m\n\u001b[32m     12\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m comparison_df = \u001b[43mbenchmark_multiple_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m display(comparison_df.sort_values(by=\u001b[33m\"\u001b[39m\u001b[33mWER\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mbenchmark_multiple_models\u001b[39m\u001b[34m(model_ids, data_dir, num_samples, batch_size)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===== Benchmarking Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m =====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Run benchmark and get results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m result = \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Collecting metrics for comparison\u001b[39;00m\n\u001b[32m     17\u001b[39m metrics = {\u001b[33m\"\u001b[39m\u001b[33mWER\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRuntime (sec)\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mruntime_sec\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m: model_id}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mrun_benchmark\u001b[39m\u001b[34m(model_id, data_dir, num_samples, batch_size)\u001b[39m\n\u001b[32m     46\u001b[39m batch_refs = references[i:i+batch_size]\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Support both batch and single-audio pipelines\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     preds = \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Ensure output is iterable\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mload_asr_model.<locals>.wrapped_pipeline\u001b[39m\u001b[34m(audio_array, sampling_rate)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_pipeline\u001b[39m(audio_array, sampling_rate=\u001b[32m16000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masr_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:283\u001b[39m, in \u001b[36mAutomaticSpeechRecognitionPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    224\u001b[39m     inputs: Union[np.ndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    225\u001b[39m     **kwargs,\n\u001b[32m    226\u001b[39m ):\n\u001b[32m    227\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m    documentation for more information.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m \u001b[33;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\pipelines\\base.py:1360\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1357\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1358\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1359\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     outputs = \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:521\u001b[39m, in \u001b[36mAutomaticSpeechRecognitionPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    519\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps == \u001b[33m\"\u001b[39m\u001b[33mword\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type == \u001b[33m\"\u001b[39m\u001b[33mseq2seq_whisper\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\generation\\utils.py:3424\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3420\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3422\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3423\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3424\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3426\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[32m   3427\u001b[39m     model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\generation\\utils.py:512\u001b[39m, in \u001b[36mGenerationMixin.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m     inputs_embeds, input_ids = \u001b[38;5;28mself\u001b[39m._cache_dependant_input_preparation(\n\u001b[32m    508\u001b[39m         input_ids, inputs_embeds, cache_position\n\u001b[32m    509\u001b[39m     )\n\u001b[32m    511\u001b[39m \u001b[38;5;66;03m# 3. Prepare base model inputs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m input_ids_key = \u001b[33m\"\u001b[39m\u001b[33mdecoder_input_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_encoder_decoder\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# if `inputs_embeds` are passed, we only want to use them in the 1st generation step for every prompt.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\CS5340\\Lib\\site-packages\\transformers\\configuration_utils.py:207\u001b[39m, in \u001b[36mPretrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    204\u001b[39m         key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m    205\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__setattr__\u001b[39m(key, value)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    209\u001b[39m         key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model_list = [\n",
    "    # \"openai/whisper-medium.en\",\n",
    "    \"openai/whisper-small.en\",\n",
    "    # \"openai/whisper-tiny.en\",\n",
    "    # \"openai/whisper-base.en\",\n",
    "    # \"facebook/wav2vec2-base-960h\",\n",
    "    \"jensenlwt/whisper-small-singlish-122k\",\n",
    "    \"mjwong/whisper-large-v3-turbo-singlish\",\n",
    "    # \"mjwong/whisper-small-singlish\",\n",
    "    # \"path/to/your/local/model\",  # Example for a local model\n",
    "]\n",
    "\n",
    "comparison_df = benchmark_multiple_models(model_list)\n",
    "display(comparison_df.sort_values(by=\"WER\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98df652accba471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc3b94-fd7e-4c60-b4bc-31c9d1146621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
